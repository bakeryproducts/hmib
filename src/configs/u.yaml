INPUTS: input
OUTPUTS: output

SPLIT: 0 # changing splits overriding config on run, i.e. python3 starter.py ++SPLITS=3
DATA:
  # root: 'preprocessed/png1024overlap/images/'
  # ann_root: 'preprocessed/png1024overlap/masks/'
  # root: 'preprocessed/rle1024/images/'
  train_imgs: 'preprocessed/rle512_overlap.5/images/'
  train_anns: 'preprocessed/rle512_overlap.5/masks/'

  valid_imgs: 'preprocessed/rle1024/images/'
  valid_anns: 'preprocessed/rle1024/masks/'
  # root: 'preprocessed/fixed1024_overlap512/images/'
  # ann_root: 'preprocessed/fixed1024_overlap512/masks/'
  # root: 'preprocessed/fixed768_overlap512/images/'
  # ann_root: 'preprocessed/fixed768_overlap512/masks/'
  # img_loader: 'partial(data.Names, suffix=".png")' # for dali mode
  # ann_loader: 'partial(data.Names, suffix=".png")'
  img_loader: 'partial(data.PngImages, suffix=".png")'
  ann_loader: 'partial(data.PngImages, suffix=".png")'
  DALI: False
  TRAIN:
    DATASETS: ['train_${SPLIT}', ]
    DATASET_ARGS:
      rate: 4 # make single epoch longer
      # rate: 50 # make single epoch longer

  VALID:
    DATASETS: ['valid_${SPLIT}', ]
    DATASET_ARGS:
      rate: 1

AUGS:
  STD: [69] # TODO: fix stats
  MEAN: [176.]
  TRAIN_RESIZE: [0]
  VALID_RESIZE: [0]
  MSR:
    scale_up: 1.0
    scale_down: 1.0
  MIXUP:
    PROB: .0
  CUTMIX:
    PROB: .0
  FMIX:
    ALPHA: 1
    REPEAT: 1
    PROB: .0

  AUG_MULTIPLICITY: 1
  # CROP: [192, 192]
  # CROP: [256,256]
  # VALCROP: [192,192]
  CROP: [384,384]
  # CROP: [512,512]
  # VALCROP: [384,384]
  # VALCROP: [512,512]
  VALCROP: [768,768]
  TRAIN:
    AUGS:
      - {m : albu.RandomCrop, args : '${AUGS.CROP}'}
      # - {m : albu.CenterCrop, args : '${AUGS.CROP}'}
      # - {m : ParamCrop, args : '${AUGS.CROP}'}
      # - {m : albu.RandomResizedCrop, args : [384,384], kwargs : {'scale' : [.5, 1] }}
      # - {m : albu.GridDistortion, kwargs : {'p': .3}}
      - {m : albu.RandomRotate90, }
      - {m : albu.HorizontalFlip, }
      # - {m : albu.ChannelShuffle, args: [.5,]}
      # - {m : albu.VerticalFlip, }
      - {m : albu.Rotate, kwargs: {'limit': 45, 'p': .5}}
      # - {m : albu.RandomGamma, kwargs : {'p': .2}} 
      # - {m : albu.ColorJitter, args: [.3, .3, .4, .1], kwargs : {'p': .25}}  # bright, contr, sat , hue
      - {m : albu.PixelDropout, kwargs : {'dropout_prob': .05 ,'p': .2}} 
      # - {m : albu.CoarseDropout, args: [4, 48, 48], kwargs : {'p': .3}} 

      - {m : sh.augmentation.ToTensor,}
  VALID:
    AUGS:
      - {m : albu.CenterCrop, args : '${AUGS.VALCROP}'}
      - {m : sh.augmentation.ToTensor,}

MODEL:
  ARCH: 'unet' # 'ssl'
  INIT_MODEL: ''
  ENCODER:
    model_name: 'resnet34'
    # model_name: 'tf_efficientnet_b2_ns'
    # model_name: 'tf_efficientnetv2_m_in21k'
    # model_name: 'resnetv2_50x1_bitm_in21k'
    # model_name: 'convnext_small_in22ft1k'
    # model_name: 'dm_nfnet_f2'
    # model_name: 'convnext_tiny'
    in_chans: 3

  DECODER:
    base:
      use_bottleneck: False
      last_scale: 4
    blocks:
      - ch: 512
        e_prehook: 'torch.nn.Upsample(scale_factor=(2,2), mode="bilinear")'
      - ch: 256
        e_prehook: 'torch.nn.Upsample(scale_factor=(2,2), mode="bilinear")'
      - ch: 128
        e_prehook: 'torch.nn.Upsample(scale_factor=(2,2), mode="bilinear")'
      # - ch: 64
      #   use_skip: False
      #   e_prehook: 'torch.nn.Upsample(scale_factor=(1,1), mode="bilinear")'
      # - ch: 32
      #   e_prehook: 'torch.nn.Upsample(scale_factor=(1,1), mode="bilinear")'
    # all_stages:

  SEGHEAD:
    out_channels: 1
    kernel_size: 1

TRAIN:
  BATCH_NUM_INSTANCES: 1
  AMP: True
  GPUS: [0,1,2,3]
  NUM_WORKERS: 3
  SAVE_STEP: 50
  SCALAR_STEP: 1
  TB_STEP: 1
  START_VAL: 0
  EPOCH_COUNT: 100
  BATCH_SIZE: 64
  EARLY_EXIT: 20
  SEED: 0 # not fixed! no touching! TODO: fix this, should be hidden in config.py
  EMA: 
    # TODO: step 
    ENABLE: True
    start: 0.97
    end: .99
    type: 'linear'

VALID:
  BATCH_SIZE: 8

LOSS:
    - name: 'seg_dice'
      weight: .8
      LOSS:
        _target_: segmentation_models_pytorch.losses.DiceLoss
        mode: 'binary'
        smooth: .0

    - name: 'seg_ce'
      weight: .2
      LOSS:
        # _target_: torch.nn.CrossEntropyLoss
        _target_: torch.nn.BCEWithLogitsLoss
        # _target_: segmentation_models_pytorch.losses.SoftCrossEntropyLoss
        # smooth_factor: .0
    - name: 'cls'
      weight: .01
      LOSS:
        _target_: torch.nn.CrossEntropyLoss
  

FEATURES:
  GRAD_CLIP: 10
  CLAMP: 10
  CLIP_MODE: 'value'
  BATCH_ACCUMULATION_STEP: 1
  USE_DS: False
  SAM:
    REDUCE: 1.
    RHO: 0.


OPTIMIZER:
  LRS: [0.000001, 0.0002, 0.0001, 1]
  # LRS: [0.00001, 0.00025, 0.0001, 1]
  # LRS: [0.000001, 0.00006, 0.00001, 0]
  
  FINE_LR:
    - name: 'seg_head'
      group_options:
        lr_scale: 1
        weight_decay: 1

  OPT:
    _target_: torch.optim.AdamW
    # _target_: timm.optim.MADGRAD
    _partial_: true
    weight_decay: 0.001

PARALLEL:
  DDP: True

hydra:
    run:
        dir: ./output/${now:%m-%d}/${now:%H-%M-%S}_${MODEL.ARCH}_${MODEL.ENCODER.model_name}

defaults: 
    - PARALLEL: _parallel
    # - TRAIN: _train
    - VALID: _valid
    - TEST: _test
    - FEATURES.SAM: _sam
    - AUGS.MIXUP: _mixup
    - _self_
