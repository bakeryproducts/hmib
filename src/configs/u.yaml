INPUTS: input
OUTPUTS: output

SPLIT: 0
DATA:
  DALI: false
  img_loader: 'partial(data.PngImages, suffix=".png")'
  ann_loader: 'partial(data.PngImages, suffix=".png")'
  TRAIN:
    DATASETS: [
      'hpa_train_kidney',
      'hpa_train_prostate',
      'hpa_train_largeintestine',
      'hpa_train_fixed_spleen',
      'hpa_train_fixed_lung',

      'hpa_extra_largeintestine',
      'hpa_extra_kidney',
      'hpa_extra_spleen',
      'hpa_extra_lung',
      'hpa_extra_prostate',

      'gtex_largeintestine', 
      'gtex_spleen',
      'gtex_kidney',
      'gtex_prostate',

      'hubmap_kidney',
      'hubmap_largeintestine',
      ]

  VALID:
    DATASETS: [
      'hpa_valid_kidney',
      'hpa_valid_prostate',
      'hpa_valid_largeintestine',
      'hpa_valid_fixed_spleen',
      'hpa_valid_fixed_lung',
    ]
  VALID_HUB:
    DATASETS: ['hubmap_test_kidney', 'hubmap_test_largeintestine']
  VALID_GTEX:
    DATASETS: [
      # 'gtex_prostate',
      'gtex_test_kidney',
      'gtex_test_spleen',
      'gtex_test_largeintestine',
    ]

AUGS:
  NORM:
    MODE: 'const'
    STD: [69] 
    MEAN: [176.]
  TRAIN_RESIZE: [0]
  VALID_RESIZE: [0]
  MSR:
    scale_up: 1.0
    scale_down: 1.0
  MIXUP:
    PROB: .0
  CUTMIX:
    PROB: .0
  FMIX:
    ALPHA: 1
    REPEAT: 1
    PROB: .3

  AUG_MULTIPLICITY: 1
  # CROP: [256,256]
  # CROP: [384,384]
  # CROP: [512,512]
  CROP: [640,640]
  # CROP: [768, 768]
  # VALCROP: [512, 512]
  # VALCROP1: [512, 512]
  # VALCROP2: [512, 512]
  # VALCROP1: [768, 768]
  VALCROP2: [768, 768]
  VALCROP1: [1152, 1152]
  # VALCROP2: [1024, 1024]
  TRAIN:
    AUGS:
      # - {m : albu.RandomCrop, args : [512,512]}
      - {m : ShiftScaleRotate, kwargs : {'shift_limit': 0.1, 'scale_limit': [-0.5, 1.0], 'rotate_limit': 30, 'border_mode': 0, 'value': 0, 'mask_value': 0, 'p': 0.5}}
      - {m : albu.RandomCrop, args : '${AUGS.CROP}'}
      # - {m : DomainStainer, kwargs : {'step': 100, 'domain': '"gtex"', 'p': .3,}}
      # - {m : albu.CenterCrop, args : '${AUGS.CROP}'}
      - {m : albu.RandomRotate90, }
      - {m : albu.HorizontalFlip, }
      - {m : ColorMeanShift, kwargs : {'p': 0.2}}
      #- {m : ColorAugs, kwargs : {'p': 0.2}}
      - {m : albu.RGBShift, args: [40, 40, 40], kwargs: {'p': 0.5}}
      # - {m : albu.GaussNoise, kwargs : {'var_limit': [10, 50] , 'p': 0.3}}
      - {m : albu.CLAHE, kwargs : {'clip_limit': 4 , 'p': 0.2}}
      - {m : NoiseAugs, kwargs : {'p': 0.5}}
      - {m : sh.augmentation.ToTensor,}
  VALID:
    AUGS:
      - {m : albu.CenterCrop, args : '${AUGS.VALCROP1}'}
      - {m : sh.augmentation.ToTensor,}
  VALID_HUB:
    AUGS:
      - {m : albu.CenterCrop, args : '${AUGS.VALCROP2}'}
      - {m : sh.augmentation.ToTensor,}
  VALID_GTEX:
    AUGS:
      - {m : albu.CenterCrop, args : '${AUGS.VALCROP2}'}
      - {m : sh.augmentation.ToTensor,}

MODEL:
  ARCH: 'unet' 
  # ARCH: 'upernet'
  INIT_MODEL: ""
  ENCODER:
    runner: 
        _target_: encoders.encoder.create_mixt
        # _target_: encoders.encoder.create_encoder
        # _target_: encoders.encoder.create_coat
        # _target_: encoders.encoder.create_swin
        _partial_: true
    # model_name: 'resnet34'
    # model_name: 'regnetx_120'
    # model_name: 'coat_lite_mini'
    # model_name: 'coat_mini'
    # model_name: 'swin'
    model_name: 'mit_b4'
    # model_name: 'tf_efficientnet_b2_ns'
    # model_name: 'tf_efficientnetv2_m_in21k'
    # model_name: 'resnetv2_50x1_bitm_in21k'
    # model_name: 'convnext_small_in22ft1k'
    # model_name: 'convnext_base_in22ft1k'
    # model_name: 'convnext_base_384_in22ft1k'
    # model_name: 'convnext_large_384_in22ft1k'
    # model_name: 'dm_nfnet_f1'
    # model_name: 'convnext_tiny'
    # img_size: 512
    in_chans: 4
    drop_rate: .4
    drop_path_rate: .4
    attn_drop_rate: .4

  DECODER:
    runner: 
        # _target_: decoders.decoder.create_segdec
        _target_: decoders.decoder.create_decoder
        _partial_: true
        # embedding_dim: 128
    base:
      use_bottleneck: False
      last_scale: 1
    blocks:
      - ch: 768
        e_prehook: 'torch.nn.Upsample(scale_factor=(2,2), mode="bilinear")'
      - ch: 256
        e_prehook: 'torch.nn.Upsample(scale_factor=(2,2), mode="bilinear")'
      - ch: 128
        e_prehook: 'torch.nn.Upsample(scale_factor=(2,2), mode="bilinear")'
      - ch: 128
        #use_skip: False
        e_prehook: 'torch.nn.Upsample(scale_factor=(2,2), mode="bilinear")'
      - ch: 64
        e_prehook: 'torch.nn.Upsample(scale_factor=(2,2), mode="bilinear")'
    # all_stages:

  SEGHEAD:
    out_channels: 5
    kernel_size: 1

TRAIN:
  BATCH_NUM_INSTANCES: 1
  AMP: True
  GPUS: [0,1,2,3]
  NUM_WORKERS: 3
  SAVE_STEP: 20
  SCALAR_STEP: 1
  TB_STEP: 1
  START_VAL: 0
  EPOCH_COUNT: 250
  BATCH_SIZE: 8
  EARLY_EXIT: 50
  SEED: 0 # not fixed! no touching! TODO: fix this, should be hidden in config.py
  EMA: 
    ENABLE: True
    start: 0.97
    end: .99
    type: 'linear'

VALID:
  BATCH_SIZE: 4
VALID_HUB:
  BATCH_SIZE: 4
VALID_GTEX:
  BATCH_SIZE: 4

LOSS:
    - name: 'seg_dice'
      weight: .6
      LOSS:
        _target_: segmentation_models_pytorch.losses.DiceLoss
        mode: 'multilabel'
        smooth: .0

    - name: 'seg_ce'
      weight: .4
      LOSS:
        # _target_: torch.nn.CrossEntropyLoss
        _target_: torch.nn.BCEWithLogitsLoss
        # _target_: segmentation_models_pytorch.losses.SoftCrossEntropyLoss
        # smooth_factor: .0
    # - name: 'cls'
    #   weight: .01
    #   LOSS:
    #     _target_: torch.nn.CrossEntropyLoss
  

FEATURES:
  GRAD_CLIP: 10
  CLAMP: 10
  CLIP_MODE: 'value'
  BATCH_ACCUMULATION_STEP: 1
  USE_DS: False
  SAM:
    REDUCE: 1.
    RHO: 0.


OPTIMIZER:
  LRS: [0.000001, 0.0001, 0.00001, 1]
  # LRS: [0.000001, 0.0008, 0.0001, 1]
  # LRS: [0.000001, 0.00008, 0.00001, 1]
  # LRS: [0.0001, 0.0008, 0.0001, 1]
  # LRS: [0.0001, 0.01, 0.001, 1]
  
  FINE_LR:
    - name: 'seg_head'
      group_options:
        lr_scale: 1
        weight_decay: .005
        # weight_decay: .1

  OPT:
    _target_: torch.optim.AdamW
    # _target_: timm.optim.MADGRAD
    _partial_: true
    weight_decay: 0.001
    # weight_decay: 0.05

PARALLEL:
  DDP: True


_postfix : ${MODEL.ARCH}_${MODEL.ENCODER.model_name}
hydra:
  run:
    dir: ./output/${now:%m-%d}/${now:%H-%M-%S}_${_postfix}
  sweep:
    dir : ./output/${now:%m-%d}/${now:%H-%M-%S}_SWEEP
    subdir: J${hydra.job.num}_${_postfix}

defaults: 
    - PARALLEL: _parallel
    - VALID: _valid
    - FEATURES.SAM: _sam
    - AUGS.MIXUP: _mixup
    - _self_
    # - DATA: datasets_0.8
    # - DATA: datasets_1.2
    # - DATA: datasets_1.2_rate
    - DATA: datasets_0.8_rate
